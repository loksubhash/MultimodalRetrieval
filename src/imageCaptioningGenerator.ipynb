{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Flexible, Encoder-Decoder Framework for Image Captioning\n",
    "### *Train, Evaluate, and Compare Captioning Models*\n",
    "\n",
    "This notebook provides a comprehensive, single-file framework to train, evaluate, and compare two distinct image captioning models. The architecture is robust, reusable, and portable, inspired by professional research pipelines.\n",
    "\n",
    "### Core Architectures for Comparison:\n",
    "The primary goal is to compare two encoder-decoder architectures:\n",
    "1.  **Classic Approach**: ResNet50 (Encoder) + LSTM (Decoder).\n",
    "2.  **State-of-the-Art Approach**: ViT (Encoder) + Pre-trained Transformer Decoder (GPT-2).\n",
    "\n",
    "### Key Architectural & Workflow Requirements:\n",
    "- **Modular, Multi-Step Notebook**: Structured into logical cells (e.g., \"Step 1: Setup\", \"Step 2: Configuration\") for clarity.\n",
    "- **Centralized Configuration**: All settings, including model choices and hyperparameters, are managed in a central `experiment_configs` dictionary.\n",
    "- **Decoupled Computation and Reporting**: The main loop handles all computation first, storing results in a dictionary. A separate, final cell generates a clean, consolidated report.\n",
    "- **Portable Evaluation**: Supports an \"evaluate-only\" mode by loading pre-trained weights from a local path or a Google Drive zip file.\n",
    "- **Data Pipeline & Vocabulary**: Includes a `Vocabulary` class to handle word-to-index mapping and special tokens (`<start>`, `<end>`, `<pad>`, `<unk>`). The DataLoader uses this to numericalize captions.\n",
    "- **Advanced Training**: Implements **teacher forcing**, **two-phase fine-tuning** with **differential learning rates**, **gradient accumulation**, **early stopping**, and **caption sampling** for robust training.\n",
    "- **Inference and Evaluation**: A `generate_caption` function produces text for evaluation. Standard captioning metrics are calculated: **BLEU, METEOR, and CIDEr**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Environment Setup and Dependency Installation**\n",
    "This cell handles the initial setup, installing necessary packages for the project. It includes the `pycocoevalcap` library and its dependency, Java, which is essential for calculating captioning metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "def run_shell_command(command, shell_mode=True):\n",
    "    \"\"\"Executes a shell command and raises an error if it fails.\"\"\"\n",
    "    try:\n",
    "        print(f\"Running command: {command}\")\n",
    "        subprocess.run(command, shell=shell_mode, check=True, capture_output=True, text=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error executing command: {command}\")\n",
    "        print(e.stderr)\n",
    "        raise\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Detects the environment and installs dependencies.\"\"\"\n",
    "    is_colab = \"google.colab\" in sys.modules\n",
    "    is_runpod = os.path.exists(\"/workspace\") or \"RUNPOD_POD_ID\" in os.environ\n",
    "    \n",
    "    if is_colab or is_runpod:\n",
    "        env_type = \"Google Colab\" if is_colab else \"RunPod\"\n",
    "        print(f\"üöÄ {env_type} environment detected. Installing dependencies...\")\n",
    "        \n",
    "        # Install Java and Zip, dependencies for pycocoevalcap and data handling\n",
    "        print(\"Installing Java and Zip...\")\n",
    "        run_shell_command(\"apt-get update && apt-get install -y openjdk-8-jre zip\")\n",
    "\n",
    "        # Forcibly reinstall compatible Torch and Transformers versions\n",
    "        print(\"Cleaning and reinstalling Torch and Transformers...\")\n",
    "        run_shell_command(\"pip uninstall -y torch torchvision torchaudio transformers accelerate\")\n",
    "        run_shell_command(\"pip install torch==2.2.2 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\")\n",
    "        run_shell_command(\"pip install transformers==4.39.3 accelerate\")\n",
    "\n",
    "        # Install other Python packages\n",
    "        print(\"Installing other Python packages...\")\n",
    "        pip_commands = [\n",
    "            \"pip install -q 'numpy<2.0'\",\n",
    "            \"pip install -q pandas timm tqdm opencv-python scikit-learn nltk albumentations tabulate wandb nbformat\",\n",
    "            \"pip install -q pycocotools pycocoevalcap kaggle matplotlib\",\n",
    "            \"pip install -U -q ipywidgets\"\n",
    "        ]\n",
    "        for cmd in pip_commands:\n",
    "            run_shell_command(cmd)\n",
    "        print(f\"‚úÖ {env_type} dependencies installed successfully.\")\n",
    "        \n",
    "        if is_colab:\n",
    "             print(\"\\nüî• IMPORTANT: Please restart the Colab runtime now for the new libraries to take effect! üî•\")\n",
    "             print(\"Go to 'Runtime' > 'Restart Session' in the menu above.\")\n",
    "             \n",
    "        return (\"colab\" if is_colab else \"runpod\"), \"/content\" if is_colab else \"/workspace\"\n",
    "    else:\n",
    "        print(\"Environment: Local machine detected.\")\n",
    "        return \"local\", os.getcwd()\n",
    "\n",
    "def setup_from_zip(zip_path, extract_to):\n",
    "    \"\"\"Unzips a results archive to the base path.\"\"\"\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"‚ö†Ô∏è Zip file not found at {zip_path}. Cannot set up from zip.\")\n",
    "        return False\n",
    "    try:\n",
    "        print(f\"Unzipping {zip_path} to {extract_to}...\")\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_to)\n",
    "        print(\"‚úÖ Unzipping complete.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to unzip results from {zip_path}. Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run setup and define base_path globally\n",
    "env_name, base_path = setup_environment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2: Main Imports and Experiment Configuration**\n",
    "This cell imports all necessary libraries and defines the central `experiment_configs` dictionary. This is where you can easily switch between models (e.g., `resnet50_lstm` vs. `vit_gpt2`), adjust hyperparameters, and enable advanced training techniques like **two-phase training**, **gradient accumulation**, and **caption sampling**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re \n",
    "import requests\n",
    "import time\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import timm\n",
    "from transformers import AutoModel, AutoTokenizer, GPT2LMHeadModel, GPT2Tokenizer, get_cosine_schedule_with_warmup\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import wandb\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "\n",
    "class BaseCFG:\n",
    "    debug = False\n",
    "    epochs = 40  # Slightly more to accommodate two-phase models\n",
    "    num_workers = 2\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    force_model_retrain = False\n",
    "    run_evaluation = True\n",
    "    evaluate_per_epoch = False # Set to True for detailed metric tracking\n",
    "    WANDB_API_KEY = \"7bbf7dc1d29a93c3cd9e115741e377d149f63ee7\" # Add your API key here for non-interactive login\n",
    "\n",
    "    model_artifacts_zip_path = None\n",
    "\n",
    "    image_size = 224\n",
    "    max_length = 40  # Increased\n",
    "    vocab_threshold = 3  # Lowered to allow more words\n",
    "\n",
    "    gradient_accumulation_steps = 1\n",
    "    early_stopping_patience = 10\n",
    "    use_caption_sampling = True\n",
    "    use_two_phase_training = True\n",
    "    phase1_epochs = 5\n",
    "    \n",
    "experiment_configs = {\n",
    "    \"resnet50_lstm\": {\n",
    "        \"models\": {\n",
    "            \"encoder_name\": \"resnet50\",\n",
    "            \"decoder_name\": \"lstm\"\n",
    "        },\n",
    "        \"hyperparameters\": {\n",
    "            \"flickr8k\": {\n",
    "                \"batch_size\": 64,\n",
    "                \"embed_dim\": 256,\n",
    "                \"hidden_dim\": 768, # 512\n",
    "                \"num_layers\": 1,\n",
    "                \"encoder_lr\": 1e-4,\n",
    "                \"decoder_lr\": 3e-4,      \n",
    "                \"weight_decay\": 5e-4,    \n",
    "                \"dropout\": 0.3,          \n",
    "                \"use_caption_sampling\": True,\n",
    "                \"early_stopping_patience\": 5, \n",
    "                \"use_two_phase_training\": True,\n",
    "                \"phase1_epochs\": 8         \n",
    "            },\n",
    "            \"flickr30k\": {\n",
    "                \"batch_size\": 96,\n",
    "                \"embed_dim\": 512, # earlier 384\n",
    "                \"hidden_dim\": 512,\n",
    "                \"num_layers\": 2,\n",
    "                \"encoder_lr\": 1e-4,\n",
    "                \"decoder_lr\": 1.5e-4,    # From current 2e-4\n",
    "                \"weight_decay\": 5e-4,   \n",
    "                \"dropout\": 0.3,         \n",
    "                \"use_caption_sampling\": True,\n",
    "                \"early_stopping_patience\": 5, \n",
    "                \"use_two_phase_training\": True,\n",
    "                \"phase1_epochs\": 10  # increased from 8      \n",
    "            }\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"vit_gpt2\": {\n",
    "        \"models\": {\n",
    "            \"encoder_name\": \"vit_base_patch16_224\",\n",
    "            \"decoder_name\": \"gpt2\"\n",
    "        },\n",
    "        \"hyperparameters\": {\n",
    "            \"flickr8k\": {\n",
    "                \"batch_size\": 64,\n",
    "                \"num_workers\": 8,\n",
    "                \"embed_dim\": 768,\n",
    "                \"encoder_lr\": 1.2e-4,  # Lowered for ViT stability\n",
    "                \"decoder_lr\": 3e-4,  # Lowered for GPT2 stability\n",
    "                \"weight_decay\": 2.5e-4,\n",
    "                \"dropout\": 0.45,      # Transformer-friendly\n",
    "                \"gradient_accumulation_steps\": 4,\n",
    "                \"use_two_phase_training\": True,\n",
    "                \"use_caption_sampling\": True,\n",
    "                \"early_stopping_patience\": 10,\n",
    "                \"max_length\": 40,\n",
    "                \"phase1_epochs\": 15\n",
    "            },\n",
    "            \"flickr30k\": {\n",
    "                \"batch_size\": 96,\n",
    "                \"num_workers\": 4,\n",
    "                \"embed_dim\": 768,\n",
    "                \"encoder_lr\": 5e-5,\n",
    "                \"decoder_lr\": 1e-4,\n",
    "                \"weight_decay\": 1e-4,\n",
    "                \"dropout\": 0.2,\n",
    "                \"gradient_accumulation_steps\": 2,\n",
    "                \"use_two_phase_training\": True,\n",
    "                \"use_caption_sampling\": True,\n",
    "                \"early_stopping_patience\": 8,\n",
    "                \"max_length\": 40,\n",
    "                \"phase1_epochs\": 10\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Login to Weights & Biases\n",
    "wandb_api_key = os.environ.get(\"WANDB_API_KEY\") or BaseCFG.WANDB_API_KEY\n",
    "try:\n",
    "    if wandb_api_key:\n",
    "        print(\"Logging into WandB using API key.\")\n",
    "        wandb.login(key=wandb_api_key)\n",
    "    else:\n",
    "        print(\"WandB API Key not found. Attempting interactive login.\")\n",
    "        wandb.login()\n",
    "except Exception as e:\n",
    "    print(f\"Could not log in to WandB: {e}\")\n",
    "    print(\"Proceeding without WandB logging.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3: Path and Data Download Utilities**\n",
    "These functions manage directory creation and handle the download and extraction of the Flickr datasets. The Flickr30k download logic has been updated to use a multi-part download from GitHub, removing the need for Kaggle API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paths(base_path, dataset_name, cfg):\n",
    "    \"\"\"Generates and creates all necessary directory paths for an experiment.\"\"\"\n",
    "    model_combo_name = f\"{dataset_name}_{cfg.encoder_name.replace('/', '-')}_{cfg.decoder_name.replace('/', '-')}\"\n",
    "    paths = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"dataset_dir\": os.path.join(base_path, \"data\", dataset_name),\n",
    "        \"image_dir\": os.path.join(base_path, \"data\", dataset_name, \"Images\"),\n",
    "        \"captions_file\": os.path.join(base_path, \"data\", dataset_name, f\"{dataset_name}_captions.csv\"),\n",
    "        \"model_save_path\": os.path.join(base_path, \"models\", f\"{model_combo_name}.pt\"),\n",
    "        \"artifact_dir\": os.path.join(base_path, \"artifacts\", model_combo_name),\n",
    "        \"vocab_path\": os.path.join(base_path, \"artifacts\", model_combo_name, \"vocab.pt\"),\n",
    "        \"history_path\": os.path.join(base_path, \"artifacts\", model_combo_name, \"train_history.pt\")\n",
    "    }\n",
    "    for path_key in [\"dataset_dir\", \"artifact_dir\"]:\n",
    "        os.makedirs(paths[path_key], exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(paths[\"model_save_path\"]), exist_ok=True)\n",
    "    return paths\n",
    "\n",
    "def download_with_progress(url, filename):\n",
    "    \"\"\"Downloads a file from a URL with a tqdm progress bar.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with requests.get(url, stream=True) as r, open(filename, 'wb') as f, tqdm(\n",
    "        unit=\"B\", unit_scale=True, unit_divisor=1024, total=int(r.headers.get('content-length', 0)),\n",
    "        desc=f\"Downloading {os.path.basename(filename)}\"\n",
    "    ) as bar:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "            bar.update(len(chunk))\n",
    "\n",
    "def download_flickr(dataset_name, target_dir):\n",
    "    \"\"\"Downloads and extracts the specified Flickr dataset.\"\"\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    print(f\"üì• Downloading {dataset_name}...\")\n",
    "    if dataset_name == 'flickr8k':\n",
    "        url = \"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/flickr8k.zip\"\n",
    "        zip_path = os.path.join(target_dir, \"flickr8k.zip\")\n",
    "        download_with_progress(url, zip_path)\n",
    "        run_shell_command(f\"unzip -q -o {zip_path} -d {target_dir}\")\n",
    "        os.remove(zip_path)\n",
    "    elif dataset_name == 'flickr30k':\n",
    "        zip_path = os.path.join(target_dir, \"flickr30k.zip\")\n",
    "        parts = [f\"flickr30k_part0{i}\" for i in range(3)]\n",
    "        urls = [f\"https://github.com/awsaf49/flickr-dataset/releases/download/v1.0/{p}\" for p in parts]\n",
    "        part_paths = [os.path.join(target_dir, p) for p in parts]\n",
    "        for url, part_path in zip(urls, part_paths):\n",
    "            download_with_progress(url, part_path)\n",
    "        \n",
    "        run_shell_command(f\"cat {' '.join(part_paths)} > {zip_path}\")\n",
    "        for part in part_paths:\n",
    "            os.remove(part)\n",
    "        run_shell_command(f\"unzip -q -o {zip_path} -d {target_dir}\")\n",
    "        os.remove(zip_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "\n",
    "def clean_caption(text):\n",
    "    \"\"\"Cleans a single caption string.\"\"\"\n",
    "    text = str(text).lower().strip()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def process_captions(raw_captions_path, final_captions_path, cfg):\n",
    "    print(f\"Processing captions from {raw_captions_path}...\")\n",
    "    if not os.path.exists(raw_captions_path):\n",
    "        print(f\"‚ùå Missing raw captions file: {raw_captions_path}\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(raw_captions_path)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df.rename(columns={\"image_name\": \"image\", \"comment\": \"caption\"}, inplace=True)\n",
    "    df.dropna(subset=[\"caption\"], inplace=True)\n",
    "    df[\"caption\"] = df[\"caption\"].astype(str).str.strip().apply(clean_caption)\n",
    "    df[\"num_tokens\"] = df[\"caption\"].apply(lambda x: len(x.split()))\n",
    "    max_tokens = cfg.max_length - 2\n",
    "    df = df[(df[\"num_tokens\"] >= 3) & (df[\"num_tokens\"] <= max_tokens)].reset_index(drop=True)\n",
    "\n",
    "    df[\"caption_number\"] = df.groupby(\"image\").cumcount()\n",
    "    df[\"id\"] = df[\"image\"].factorize()[0]\n",
    "    df = df[[\"image\", \"caption_number\", \"caption\", \"id\"]]\n",
    "\n",
    "    df.to_csv(final_captions_path, index=False)\n",
    "    print(f\"\\n‚úÖ Preprocessing DONE\")\n",
    "    print(f\"üìù Total captions: {len(df)}\")\n",
    "    print(f\"üî§ Avg length: {df['caption'].apply(lambda x: len(x.split())).mean():.2f} tokens\")\n",
    "    print(f\"üìÑ Saved: {final_captions_path}\")\n",
    "\n",
    "\n",
    "def prepare_dataset(config, cfg):\n",
    "    \"\"\"Main function to ensure dataset is downloaded and processed.\"\"\"\n",
    "    dataset_name = config[\"dataset_name\"]\n",
    "    dataset_dir = config[\"dataset_dir\"]\n",
    "    image_dir = config[\"image_dir\"]\n",
    "    final_captions_file = config[\"captions_file\"]\n",
    "    raw_captions_path = os.path.join(dataset_dir, 'captions.txt')\n",
    "\n",
    "    # Stage 1: Check if the FINAL processed file exists.\n",
    "    if os.path.exists(final_captions_file) and not BaseCFG.force_model_retrain:\n",
    "        print(f\"‚úÖ Dataset '{dataset_name}' found and already processed. Skipping preparation.\")\n",
    "        return\n",
    "\n",
    "    # Stage 2: Check if the RAW data exists.\n",
    "    if not (os.path.exists(image_dir) and os.path.exists(raw_captions_path)) or BaseCFG.force_model_retrain:\n",
    "        print(f\"Raw dataset '{dataset_name}' not found or retraining forced. Starting download...\")\n",
    "        download_flickr(dataset_name, dataset_dir)\n",
    "        # After download, we might have a nested folder, so let's restructure\n",
    "        if not os.path.exists(image_dir) and os.path.exists(os.path.join(dataset_dir, 'flickr8k')):\n",
    "            nested_dir = os.path.join(dataset_dir, 'flickr8k')\n",
    "            for item in os.listdir(nested_dir):\n",
    "                shutil.move(os.path.join(nested_dir, item), dataset_dir)\n",
    "            shutil.rmtree(nested_dir)\n",
    "        if not os.path.exists(image_dir) and os.path.exists(os.path.join(dataset_dir, 'flickr30k-images')):\n",
    "            shutil.move(os.path.join(dataset_dir, 'flickr30k-images'), image_dir)\n",
    "    else:\n",
    "        print(f\"Found raw dataset '{dataset_name}'. Skipping download.\")\n",
    "\n",
    "    # Stage 3: Process the raw data.\n",
    "    print(f\"Processing raw dataset...\")\n",
    "    process_captions(raw_captions_path, final_captions_file, cfg)\n",
    "    print(f\"Dataset '{dataset_name}' is now ready for use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4: Vocabulary, Dataset, and DataLoader**\n",
    "This is a critical section with major changes from the retrieval notebook.\n",
    "- **`Vocabulary` class**: Builds a word mapping from the training captions, handling special tokens.\n",
    "- **`CaptioningDataset`**: Prepares each image and its corresponding numericalized caption.\n",
    "- **`get_transforms`**: Standard image augmentation for training and resizing for validation.\n",
    "- **`collate_fn`**: A custom function for the DataLoader that pads caption sequences in each batch to be the same length. This is essential for batch processing in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.pad_idx = self.stoi[\"<PAD>\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in word_tokenize(str(sentence)):\n",
    "                frequencies[word] += 1\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = word_tokenize(str(text).lower())\n",
    "        return [\n",
    "            self.stoi[\"<SOS>\"]] + [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] \n",
    "            for token in tokenized_text\n",
    "        ] + [self.stoi[\"<EOS>\"]]\n",
    "\n",
    "class CaptioningDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, vocab_or_tokenizer, transforms, cfg):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.vocab_or_tokenizer = vocab_or_tokenizer\n",
    "        self.transforms = transforms\n",
    "        self.cfg = cfg\n",
    "        self.is_gpt2 = isinstance(self.vocab_or_tokenizer, GPT2Tokenizer)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.df.caption.iloc[idx]\n",
    "        image_id = self.df.image.iloc[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_id)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = np.array(image)\n",
    "            image = self.transforms(image=image)['image']\n",
    "        except (FileNotFoundError, OSError):\n",
    "            print(f\"Warning: Could not load image {image_path}. Skipping.\")\n",
    "            return None\n",
    "\n",
    "        if self.is_gpt2:\n",
    "            # For GPT-2, use its tokenizer\n",
    "            encoding = self.vocab_or_tokenizer(caption, padding=\"max_length\", truncation=True, max_length=self.cfg.max_length, return_tensors=\"pt\")\n",
    "            caption_tensor = encoding['input_ids'].squeeze(0)\n",
    "        else:\n",
    "            # For LSTM, use the custom vocabulary\n",
    "            numericalized_caption = self.vocab_or_tokenizer.numericalize(caption)\n",
    "            caption_tensor = torch.tensor(numericalized_caption)\n",
    "        \n",
    "        return image, caption_tensor, image_id\n",
    "    \n",
    "    def update_df(self, new_df):\n",
    "        \"\"\"Allows the Trainer to update the dataframe, used for caption sampling.\"\"\"\n",
    "        self.df = new_df\n",
    "\n",
    "def get_transforms(cfg, mode=\"train\"):\n",
    "    if mode == \"train\":\n",
    "        return A.Compose([\n",
    "            A.Resize(cfg.image_size, cfg.image_size),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ColorJitter(p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(cfg.image_size, cfg.image_size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "class Collate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # Filter out None values from missing images\n",
    "        batch = [b for b in batch if b is not None]\n",
    "        if not batch:\n",
    "            return None, None, None\n",
    "        \n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=True, padding_value=self.pad_idx)\n",
    "        image_ids = [item[2] for item in batch]\n",
    "        \n",
    "        return imgs, targets, image_ids\n",
    "    \n",
    "def make_train_valid_dfs(config):\n",
    "    df = pd.read_csv(config['captions_file'])\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    \n",
    "    image_files_in_dir = set(os.listdir(config['image_dir']))\n",
    "    df = df[df['image'].isin(image_files_in_dir)].reset_index(drop=True)\n",
    "    \n",
    "    # For splitting, we use unique images. For training, we use all 5 captions per image.\n",
    "    unique_images = df['image'].unique()\n",
    "    np.random.seed(42)\n",
    "    train_mask = np.random.rand(len(unique_images)) < 0.9\n",
    "    train_images = unique_images[train_mask]\n",
    "    valid_images = unique_images[~train_mask]\n",
    "    \n",
    "    train_df = df[df['image'].isin(train_images)].reset_index(drop=True)\n",
    "    # For validation, we keep all 5 captions to calculate metrics correctly\n",
    "    valid_df = df[df['image'].isin(valid_images)].reset_index(drop=True)\n",
    "    \n",
    "    return train_df, valid_df\n",
    "\n",
    "def build_loaders(df, image_dir, vocab_or_tokenizer, mode, cfg, shuffle=True):\n",
    "    transforms = get_transforms(cfg, mode)\n",
    "    dataset = CaptioningDataset(df, image_dir, vocab_or_tokenizer, transforms, cfg)\n",
    "    \n",
    "    pad_idx = vocab_or_tokenizer.pad_token_id if isinstance(vocab_or_tokenizer, GPT2Tokenizer) else vocab_or_tokenizer.pad_idx\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=cfg.batch_size,\n",
    "        num_workers=cfg.num_workers,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=Collate(pad_idx=pad_idx),\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5: Model Definitions**\n",
    "This cell defines the core Encoder-Decoder models.\n",
    "- **`Encoder`**: A wrapper for `timm` models (ResNet50, ViT) to produce image features.\n",
    "- **`Decoder`**: Separate implementations for the LSTM and the GPT-2 Transformer decoders.\n",
    "- **`EncoderDecoder`**: The main model that combines an encoder and a decoder. It includes a `forward` method for training and a `generate_caption` method for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, model_name, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.dropout(self.embedding(captions))\n",
    "        # Correctly expand initial hidden state for multi-layer LSTMs\n",
    "        h0 = features.unsqueeze(0).repeat(self.lstm.num_layers, 1, 1)\n",
    "        c0 = features.unsqueeze(0).repeat(self.lstm.num_layers, 1, 1)\n",
    "        hiddens, _ = self.lstm(embeddings, (h0, c0))\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim, model_name=\"gpt2\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.model.config.pad_token_id = self.model.config.eos_token_id\n",
    "        \n",
    "        # A linear layer to project ViT's embedding dim to GPT-2's if they differ\n",
    "        self.projection = nn.Linear(embed_dim, self.model.config.n_embd)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        projected_features = self.projection(features).unsqueeze(1)\n",
    "        caption_embeddings = self.model.transformer.wte(captions)\n",
    "        inputs_embeds = torch.cat([projected_features, caption_embeddings], dim=1)\n",
    "        attention_mask = torch.ones(inputs_embeds.shape[:2], dtype=torch.long, device=features.device)\n",
    "        \n",
    "        outputs = self.model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Drop the prediction for the initial image feature token to align sequences\n",
    "        return logits[:, :-1, :]\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, cfg, vocab_size_or_tokenizer):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.encoder = Encoder(cfg.encoder_name)\n",
    "        encoder_output_dim = self.encoder.model.num_features\n",
    "        \n",
    "        if cfg.decoder_name == \"lstm\":\n",
    "            # For LSTM, project encoder output to match decoder's hidden_dim\n",
    "            self.feature_proj = nn.Linear(encoder_output_dim, cfg.hidden_dim)\n",
    "            self.decoder = LSTMDecoder(cfg.embed_dim, cfg.hidden_dim, vocab_size_or_tokenizer, cfg.num_layers, cfg.dropout)\n",
    "        elif cfg.decoder_name == \"gpt2\":\n",
    "            # GPT-2 decoder has its own projection layer\n",
    "            self.decoder = TransformerDecoder(encoder_output_dim)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown decoder: {cfg.decoder_name}\")\n",
    "\n",
    "    def set_encoder_trainable(self, trainable=True):\n",
    "        \"\"\"Helper function to freeze/unfreeze the encoder's weights.\"\"\"\n",
    "        print(f\"Setting encoder trainability to: {trainable}\")\n",
    "        if self.cfg.encoder_name.startswith('resnet'):\n",
    "            # Freeze all layers initially\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            # If fine-tuning (Phase 2), unfreeze the final block\n",
    "            if trainable:\n",
    "                print(\"  - Unfreezing final block (layer4) of ResNet for fine-tuning.\")\n",
    "                for param in self.encoder.model.layer4.parameters():\n",
    "                    param.requires_grad = True\n",
    "        else: # Default behavior for other models like ViT\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = trainable\n",
    "            \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        if self.cfg.decoder_name == \"lstm\":\n",
    "            features = self.feature_proj(features)\n",
    "            outputs = self.decoder(features, captions)\n",
    "        else: # GPT-2\n",
    "            outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def generate_caption(self, image, vocab=None, max_length=30):\n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image = image.unsqueeze(0).to(self.cfg.device)\n",
    "            features = self.encoder(image)\n",
    "            \n",
    "            if self.cfg.decoder_name == \"lstm\":\n",
    "                result_caption = []\n",
    "                features = self.feature_proj(features)\n",
    "                states = (features.unsqueeze(0).repeat(self.decoder.lstm.num_layers, 1, 1), \n",
    "                          features.unsqueeze(0).repeat(self.decoder.lstm.num_layers, 1, 1))\n",
    "                inputs = self.decoder.embedding(torch.tensor([vocab.stoi[\"<SOS>\"]]).to(self.cfg.device)).unsqueeze(1)\n",
    "                \n",
    "                for _ in range(max_length):\n",
    "                    hiddens, states = self.decoder.lstm(inputs, states)\n",
    "                    outputs = self.decoder.linear(hiddens.squeeze(1))\n",
    "                    predicted_idx = outputs.argmax(1)\n",
    "                    inputs = self.decoder.embedding(predicted_idx).unsqueeze(1)\n",
    "                    \n",
    "                    if predicted_idx.item() == vocab.stoi[\"<EOS>\"]:\n",
    "                        break\n",
    "                    result_caption.append(vocab.itos[predicted_idx.item()])\n",
    "                return \" \".join(result_caption)\n",
    "            else: # GPT-2\n",
    "                features = self.decoder.projection(features).unsqueeze(1)\n",
    "                attention_mask = torch.ones(features.shape[:2], dtype=torch.long, device=features.device)\n",
    "                output_ids = self.decoder.model.generate(\n",
    "                    inputs_embeds=features, \n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=max_length, \n",
    "                    num_beams=5, \n",
    "                    early_stopping=True, \n",
    "                    pad_token_id=self.decoder.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.decoder.tokenizer.eos_token_id\n",
    "                )\n",
    "                caption = self.decoder.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "                return caption.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6: Trainer and Utilities**\n",
    "- **`AvgMeter`**: A simple utility for tracking average metrics.\n",
    "- **`Trainer`**: Manages the training and validation loops. The loops are adapted for a sequence-to-sequence task, calculating loss at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"): self.name, self.avg, self.sum, self.count = name, 0, 0, 0\n",
    "    def update(self, val, n=1): self.sum += val * n; self.count += n; self.avg = self.sum / self.count\n",
    "    def __repr__(self): return f\"{self.name}: {self.avg:.4f}\"\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, loss_fn, optimizer, scheduler, cfg):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.cfg = cfg\n",
    "        self.device = cfg.device\n",
    "\n",
    "    def _train_one_epoch(self, train_loader):\n",
    "        loss_meter = AvgMeter()\n",
    "        self.model.train()\n",
    "        progress_bar = tqdm(train_loader, total=len(train_loader), desc=\"Training\")\n",
    "        self.optimizer.zero_grad() # Reset gradients at the start of the epoch\n",
    "        \n",
    "        for i, (images, captions, _) in enumerate(progress_bar):\n",
    "            if images is None: continue\n",
    "            images, captions = images.to(self.device), captions.to(self.device)\n",
    "            \n",
    "            outputs = self.model(images, captions[:, :-1])\n",
    "            loss = self.loss_fn(outputs.reshape(-1, outputs.shape[2]), captions[:, 1:].reshape(-1))\n",
    "            \n",
    "            # Gradient Accumulation\n",
    "            loss = loss / self.cfg.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (i + 1) % self.cfg.gradient_accumulation_steps == 0:\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "                if self.scheduler: self.scheduler.step()\n",
    "                \n",
    "            loss_meter.update(loss.item() * self.cfg.gradient_accumulation_steps, images.size(0))\n",
    "            progress_bar.set_postfix(train_loss=loss_meter.avg, lr=self.optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "        return loss_meter\n",
    "\n",
    "    def _valid_one_epoch(self, valid_loader):\n",
    "        loss_meter = AvgMeter()\n",
    "        self.model.eval()\n",
    "        progress_bar = tqdm(valid_loader, total=len(valid_loader), desc=\"Validation\")\n",
    "        with torch.no_grad():\n",
    "            for images, captions, _ in progress_bar:\n",
    "                if images is None: continue\n",
    "                images, captions = images.to(self.device), captions.to(self.device)\n",
    "                \n",
    "                outputs = self.model(images, captions[:, :-1])\n",
    "                loss = self.loss_fn(outputs.reshape(-1, outputs.shape[2]), captions[:, 1:].reshape(-1))\n",
    "                \n",
    "                loss_meter.update(loss.item(), images.size(0))\n",
    "                progress_bar.set_postfix(valid_loss=loss_meter.avg)\n",
    "        return loss_meter\n",
    "\n",
    "    def fit(self, train_loader, valid_loader, config, start_epoch=0):\n",
    "        best_loss = float('inf')\n",
    "        history = {\"train_loss\": [], \"valid_loss\": [], \"epoch_times\": []}\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(start_epoch, self.cfg.epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            print(f\"\\nEpoch: {epoch + 1}/{self.cfg.epochs}\")\n",
    "            \n",
    "            # Caption Sampling\n",
    "            current_train_loader = train_loader\n",
    "            if self.cfg.use_caption_sampling:\n",
    "                print(\"Sampling one caption per image for training this epoch...\")\n",
    "                sampled_df = train_loader.dataset.df.groupby('image').sample(1).reset_index(drop=True)\n",
    "                #current_train_loader = build_loaders(sampled_df, train_loader.dataset.image_dir, train_loader.dataset.vocab_or_tokenizer, 'train', {}, self.cfg, shuffle=True)\n",
    "                current_train_loader = build_loaders(sampled_df, train_loader.dataset.image_dir, train_loader.dataset.vocab_or_tokenizer, 'train', self.cfg, shuffle=True)\n",
    "            \n",
    "            train_loss = self._train_one_epoch(current_train_loader)\n",
    "            valid_loss = self._valid_one_epoch(valid_loader)\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            epoch_duration = epoch_end_time - epoch_start_time\n",
    "            history['train_loss'].append(train_loss.avg)\n",
    "            history['valid_loss'].append(valid_loss.avg)\n",
    "            history['epoch_times'].append(epoch_duration)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} | Train Loss: {train_loss.avg:.4f} | Valid Loss: {valid_loss.avg:.4f} | Time: {epoch_duration:.2f}s\")\n",
    "            \n",
    "            if self.cfg.evaluate_per_epoch:\n",
    "                # Simplified evaluation (BLEU-4 only) for per-epoch tracking\n",
    "                temp_scores, _, _ = generate_and_evaluate(self.model, valid_loader, train_loader.dataset.vocab_or_tokenizer, config)\n",
    "                wandb.log({\"val_bleu4\": temp_scores.get(\"Bleu_4\", 0), \"epoch\": epoch})\n",
    "\n",
    "            wandb.log({\"train_loss\": train_loss.avg, \"valid_loss\": valid_loss.avg, \"epoch\": epoch})\n",
    "\n",
    "            if valid_loss.avg < best_loss:\n",
    "                best_loss = valid_loss.avg\n",
    "                torch.save({'epoch': epoch + 1, 'model_state_dict': self.model.state_dict(), 'optimizer_state_dict': self.optimizer.state_dict()}, self.cfg.model_save_path)\n",
    "                print(f\"Saved Best Model! Validation Loss: {best_loss:.4f}\")\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"Validation loss did not improve. Patience: {patience_counter}/{self.cfg.early_stopping_patience}\")\n",
    "                if patience_counter >= self.cfg.early_stopping_patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "        \n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7: Evaluation and Reporting Functions**\n",
    "These functions handle the caption generation for the entire validation set and then use the `pycocoevalcap` library to compute standard metrics.\n",
    "- **`generate_and_evaluate`**: Iterates through the validation loader, generates a caption for each image, and stores the results.\n",
    "- **`get_coco_scores`**: Formats the generated captions and ground truths into the required JSON structure and runs the COCO evaluation scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_evaluate(model, dataloader, vocab, config):\n",
    "    \"\"\"Generates captions for a dataloader and prepares data for COCO evaluation.\"\"\"\n",
    "    print(f\"Generating captions for {config['dataset_name']} validation set...\")\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    ground_truths = []\n",
    "    image_ids_processed = set()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, captions_gt, image_ids in tqdm(dataloader, desc=\"Generating Captions\"):\n",
    "            if images is None: continue\n",
    "            images = images.to(BaseCFG.device)\n",
    "            \n",
    "            for i in range(images.size(0)):\n",
    "                image_id = image_ids[i]\n",
    "                \n",
    "                if image_id not in image_ids_processed:\n",
    "                    generated_caption = model.generate_caption(images[i], vocab)\n",
    "                    results.append({\"image_id\": image_id, \"caption\": generated_caption})\n",
    "                    image_ids_processed.add(image_id)\n",
    "    \n",
    "    # Prepare ground truths from the validation dataframe\n",
    "    valid_df = dataloader.dataset.df\n",
    "    annotations = []\n",
    "    images_info = []\n",
    "    ann_id_counter = 1\n",
    "    for img_id in image_ids_processed:\n",
    "        images_info.append({\"id\": img_id})\n",
    "        captions_for_image = valid_df[valid_df['image'] == img_id]['caption'].tolist()\n",
    "        for cap in captions_for_image:\n",
    "            annotations.append({\"image_id\": img_id, \"id\": ann_id_counter, \"caption\": cap})\n",
    "            ann_id_counter += 1\n",
    "    \n",
    "    ground_truths = {\n",
    "        \"info\": {\"description\": \"Ground-truth captions for evaluation\"},\n",
    "        \"images\": images_info,\n",
    "        \"licenses\": [],\n",
    "        \"annotations\": annotations,\n",
    "        \"type\": \"captions\"\n",
    "    }\n",
    "    \n",
    "    # Calculate scores\n",
    "    scores = get_coco_scores(results, ground_truths, config[\"artifact_dir\"])\n",
    "    return scores, results, ground_truths\n",
    "\n",
    "def get_coco_scores(res, gts, artifact_dir):\n",
    "    \"\"\"Uses pycocoevalcap to calculate captioning metrics.\"\"\"\n",
    "    # Ensure artifact directory exists\n",
    "    os.makedirs(artifact_dir, exist_ok=True)\n",
    "    res_file = os.path.join(artifact_dir, \"results.json\")\n",
    "    gts_file = os.path.join(artifact_dir, \"ground_truths.json\")\n",
    "    \n",
    "    with open(res_file, \"w\") as f:\n",
    "        json.dump(res, f)\n",
    "        \n",
    "    with open(gts_file, \"w\") as f:\n",
    "        json.dump(gts, f)\n",
    "\n",
    "    coco = COCO(gts_file)\n",
    "    coco_res = coco.loadRes(res_file)\n",
    "\n",
    "    coco_eval = COCOEvalCap(coco, coco_res)\n",
    "    coco_eval.evaluate()\n",
    "\n",
    "    return coco_eval.eval\n",
    "\n",
    "def get_parameter_counts(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 8: The Main Pipeline Function**\n",
    "This function orchestrates the entire process for a single experiment: data preparation, vocabulary building, model training (or loading), evaluation, and result aggregation. It returns a dictionary containing all results and artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(config, cfg):\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"STARTING PIPELINE FOR: {config['dataset_name'].upper()}\")\n",
    "    print(f\"With model: {cfg.encoder_name} + {cfg.decoder_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # --- Setup: Data & Vocabulary ---\n",
    "    print(\"\\nSetting up datasets and vocabulary...\")\n",
    "    prepare_dataset(config, cfg)\n",
    "    train_df, valid_df = make_train_valid_dfs(config)\n",
    "    \n",
    "    if train_df is None or valid_df is None or train_df.empty or valid_df.empty:\n",
    "        print(\"Could not create dataframes. Aborting.\")\n",
    "        return None\n",
    "        \n",
    "    # --- Vocabulary or Tokenizer ---\n",
    "    if cfg.decoder_name == 'gpt2':\n",
    "        print(\"Using GPT-2 Tokenizer.\")\n",
    "        vocab_or_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "        vocab_or_tokenizer.pad_token = vocab_or_tokenizer.eos_token\n",
    "        vocab_size_or_tokenizer = None # Not needed for GPT-2 model init\n",
    "    else:\n",
    "        print(\"Building custom vocabulary...\")\n",
    "        vocab = Vocabulary(freq_threshold=cfg.vocab_threshold)\n",
    "        if os.path.exists(config[\"vocab_path\"]) and not cfg.force_model_retrain:\n",
    "            print(\"Loading existing vocabulary...\")\n",
    "            vocab = torch.load(config[\"vocab_path\"], weights_only=False)\n",
    "        else:\n",
    "            print(\"Building new vocabulary...\")\n",
    "            vocab.build_vocabulary(train_df.caption.tolist())\n",
    "            torch.save(vocab, config[\"vocab_path\"])\n",
    "        vocab_size_or_tokenizer = len(vocab)\n",
    "        vocab_or_tokenizer = vocab\n",
    "        print(f\"Vocabulary size: {vocab_size_or_tokenizer}\")\n",
    "\n",
    "    # Build data loaders\n",
    "    train_loader = build_loaders(train_df, config['image_dir'], vocab_or_tokenizer, 'train', cfg, shuffle=True)\n",
    "    valid_loader = build_loaders(valid_df, config['image_dir'], vocab_or_tokenizer, 'valid', cfg, shuffle=False)\n",
    "    \n",
    "    # --- Model Creation and Loading ---\n",
    "    print(\"\\nCreating model...\")\n",
    "    model = EncoderDecoder(cfg, vocab_size_or_tokenizer).to(cfg.device)\n",
    "    cfg.model_save_path = config['model_save_path'] # Pass save path to Trainer\n",
    "    \n",
    "    training_history = None\n",
    "    total_training_duration = 0\n",
    "    \n",
    "    if os.path.exists(cfg.model_save_path) and not cfg.force_model_retrain:\n",
    "        print(f\"Model found at '{cfg.model_save_path}'. Loading weights...\")\n",
    "        checkpoint = torch.load(cfg.model_save_path, map_location=cfg.device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        if os.path.exists(config['history_path']):\n",
    "            training_history = torch.load(config['history_path'])\n",
    "    else:\n",
    "        # --- Training --- #\n",
    "        start_time = time.time()\n",
    "        if cfg.use_two_phase_training:\n",
    "            # Phase 1: Train only the decoder\n",
    "            print(\"--- Starting Two-Phase Training: Phase 1 (Decoder Only) ---\")\n",
    "            model.set_encoder_trainable(False)\n",
    "            decoder_params = [p for p in model.parameters() if p.requires_grad]\n",
    "            optimizer = torch.optim.Adam(decoder_params, lr=cfg.decoder_lr, weight_decay=cfg.weight_decay)\n",
    "            trainer = Trainer(model, nn.CrossEntropyLoss(ignore_index=vocab_or_tokenizer.pad_token_id), optimizer, None, cfg)\n",
    "            cfg.epochs = cfg.phase1_epochs # Temporarily set epochs for phase 1\n",
    "            history1 = trainer.fit(train_loader, valid_loader, config)\n",
    "            \n",
    "            # Phase 2: Train the full model with differential learning rates\n",
    "            print(\"--- Two-Phase Training: Phase 2 (Full Model with Differential LRs) ---\")\n",
    "            model.set_encoder_trainable(True)\n",
    "            optimizer_params = [\n",
    "                {\"params\": model.encoder.parameters(), \"lr\": cfg.encoder_lr},\n",
    "                {\"params\": model.decoder.parameters(), \"lr\": cfg.decoder_lr}\n",
    "            ]\n",
    "            optimizer = torch.optim.Adam(optimizer_params, weight_decay=cfg.weight_decay)\n",
    "            trainer = Trainer(model, nn.CrossEntropyLoss(ignore_index=vocab_or_tokenizer.pad_token_id), optimizer, None, cfg)\n",
    "            cfg.epochs = BaseCFG.epochs # Reset to original epochs\n",
    "            history2 = trainer.fit(train_loader, valid_loader, config, start_epoch=cfg.phase1_epochs)\n",
    "            \n",
    "            # Combine histories\n",
    "            training_history = {k: history1[k] + history2[k] for k in history1}\n",
    "        else:\n",
    "            # Standard one-phase training\n",
    "            print(\"--- Starting Standard Training ---\")\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=cfg.decoder_lr, weight_decay=cfg.weight_decay)\n",
    "            trainer = Trainer(model, nn.CrossEntropyLoss(ignore_index=vocab_or_tokenizer.pad_token_id), optimizer, None, cfg)\n",
    "            training_history = trainer.fit(train_loader, valid_loader, config)\n",
    "            \n",
    "        total_training_duration = time.time() - start_time\n",
    "        torch.save(training_history, config['history_path'])\n",
    "\n",
    "    if not cfg.run_evaluation:\n",
    "        print(\"Skipping evaluation as per configuration.\")\n",
    "        return {\"history\": training_history, \"model\": model, \"config\": config, \"cfg\": cfg, \"vocab\": vocab_or_tokenizer, \"train_loader_len\": len(train_loader)}\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    print(\"\\nStarting evaluation...\")\n",
    "    scores, generated_captions, ground_truths = generate_and_evaluate(model, valid_loader, vocab_or_tokenizer, config)\n",
    "        \n",
    "    print(f\"\\nPIPELINE FOR {config['dataset_name'].upper()} COMPLETE\")\n",
    "    return {\n",
    "        \"history\": training_history,\n",
    "        \"metrics\": scores,\n",
    "        \"generated_captions\": generated_captions,\n",
    "        \"ground_truths\": ground_truths,\n",
    "        \"model\": model, \n",
    "        \"config\": config, \n",
    "        \"cfg\": cfg,\n",
    "        \"vocab\": vocab_or_tokenizer,\n",
    "        \"duration\": total_training_duration,\n",
    "        \"train_loader_len\": len(train_loader)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 9: Main Execution Loop**\n",
    "This cell runs the main pipeline. It iterates through the `experiment_configs`, sets up the configuration for each run, and calls the `run_pipeline` function. All results are collected in the `results_history` dictionary for the final reporting step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    results_history = {}\n",
    "    \n",
    "    if BaseCFG.model_artifacts_zip_path and setup_from_zip(BaseCFG.model_artifacts_zip_path, base_path):\n",
    "        print(\"\\nüì¶ Switched to 'Evaluation from Zip' mode.\")\n",
    "        BaseCFG.force_model_retrain = False\n",
    "    \n",
    "    experiments_to_run = [\"vit_gpt2\"] # , \"resnet50_lstm\"\n",
    "    datasets_to_process = [\"flickr8k\"] # , \"flickr30k\"\n",
    "\n",
    "    for exp_name in experiments_to_run:\n",
    "        if exp_name not in experiment_configs:\n",
    "            print(f\"Skipping unknown experiment: {exp_name}\")\n",
    "            continue\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"                RUNNING EXPERIMENT: {exp_name.upper()}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        exp_params = experiment_configs[exp_name]\n",
    "        \n",
    "        for dataset_name in datasets_to_process:\n",
    "            base_cfg_dict = {k: v for k, v in BaseCFG.__dict__.items() if not k.startswith('__')}\n",
    "            # Overwrite base config with experiment-specific ones\n",
    "            hyperparams = {**base_cfg_dict, **exp_params.get(\"hyperparameters\", {}).get(dataset_name, {})}\n",
    "            combined_params = {**hyperparams, **exp_params[\"models\"]}\n",
    "            cfg = SimpleNamespace(**combined_params)\n",
    "            \n",
    "            path_config = generate_paths(base_path, dataset_name, cfg)\n",
    "            \n",
    "            try:\n",
    "                wandb.init(\n",
    "                    project=\"image-captioning-experiments\",\n",
    "                    name=f\"{exp_name}-{dataset_name}-{int(time.time())}\",\n",
    "                    config=vars(cfg)\n",
    "                )\n",
    "                run_results = run_pipeline(path_config, cfg)\n",
    "                if run_results:\n",
    "                    if dataset_name not in results_history:\n",
    "                        results_history[dataset_name] = {}\n",
    "                    results_history[dataset_name][exp_name] = run_results\n",
    "                    wandb.log(run_results['metrics'])\n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå An error occurred during the pipeline for {exp_name} on {dataset_name}.\")\n",
    "                print(f\"Error: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            finally:\n",
    "                wandb.finish()\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 10: Final Report Generation**\n",
    "This final, decoupled cell iterates through the collected results and generates all plots, tables, and qualitative examples in one uninterrupted flow to prevent rendering issues in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(history, exp_name, dataset_name):\n",
    "    if not history or 'train_loss' not in history or 'valid_loss' not in history:\n",
    "        print(\"No training history found to plot.\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['valid_loss'], label='Validation Loss')\n",
    "    plt.title(f'Loss Curves for {exp_name} on {dataset_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def display_metrics_table(metrics):\n",
    "    if not metrics:\n",
    "        print(\"No metrics to display.\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame([metrics])\n",
    "    df = df.round(3)\n",
    "    display(Markdown(df.to_markdown(index=False)))\n",
    "\n",
    "def display_performance_summary(run_data):\n",
    "    model = run_data['model']\n",
    "    training_history = run_data['history']\n",
    "    total_training_duration = run_data.get('duration', 0)\n",
    "    train_loader_len = run_data.get('train_loader_len', 0)\n",
    "    cfg = run_data['cfg']\n",
    "    config = run_data['config']\n",
    "\n",
    "    if training_history:\n",
    "        total_epochs_trained = len(training_history['train_loss'])\n",
    "        avg_epoch_time = sum(training_history['epoch_times']) / len(training_history['epoch_times']) if training_history.get('epoch_times') else 0\n",
    "        iterations_per_epoch = train_loader_len\n",
    "        avg_iteration_time = avg_epoch_time / iterations_per_epoch if iterations_per_epoch > 0 else 0\n",
    "    else:\n",
    "        total_epochs_trained = \"N/A (Loaded from checkpoint)\"\n",
    "        avg_epoch_time = 0\n",
    "        avg_iteration_time = 0\n",
    "\n",
    "    # Parameter breakdown\n",
    "    encoder_params = sum(p.numel() for p in model.encoder.parameters() if p.requires_grad)\n",
    "    decoder_params = sum(p.numel() for p in model.decoder.parameters() if p.requires_grad)\n",
    "    proj_params = 0\n",
    "    if hasattr(model, 'feature_proj'):\n",
    "        proj_params = sum(p.numel() for p in model.feature_proj.parameters() if p.requires_grad)\n",
    "    \n",
    "    summary_md = f\"\"\"**Performance Summary for {config['dataset_name']} - {cfg.encoder_name} + {cfg.decoder_name}**\n",
    "| Metric | Value |\n",
    "| :--- | :--- |\n",
    "| **GPU Used** | {torch.cuda.get_device_name(0) if cfg.device.type == 'cuda' else 'CPU'} |\n",
    "| **Total Parameters** | {sum(p.numel() for p in model.parameters()):,} |\n",
    "| **Trainable Parameters** | {sum(p.numel() for p in model.parameters() if p.requires_grad):,} |\n",
    "| | |\n",
    "| **Trainable Breakdown** | |\n",
    "| &nbsp; &nbsp; Encoder | {encoder_params:,} |\n",
    "| &nbsp; &nbsp; Decoder | {decoder_params:,} |\n",
    "\"\"\"\n",
    "    if proj_params > 0:\n",
    "        summary_md += f\"| &nbsp; &nbsp; Projection Head | {proj_params:,} |\\n\"\n",
    "\n",
    "    summary_md += f\"\"\"| | |\n",
    "| **Training Details** | |\n",
    "| &nbsp; &nbsp; Total Epochs Trained | {total_epochs_trained} |\n",
    "| &nbsp; &nbsp; Batch Size | {cfg.batch_size} |\n",
    "| &nbsp; &nbsp; Decoder LR | {cfg.decoder_lr} |\n",
    "| &nbsp; &nbsp; Encoder LR | {cfg.encoder_lr} |\n",
    "| &nbsp; &nbsp; Optimizer | AdamW |\n",
    "| &nbsp; &nbsp; Vocab Size | {len(run_data['vocab'])} |\n",
    "| &nbsp; &nbsp; Dropout | {cfg.dropout} |\n",
    "\"\"\"\n",
    "    if cfg.decoder_name == 'lstm':\n",
    "        summary_md += f\"| &nbsp; &nbsp; LSTM Hidden Size | {cfg.hidden_dim} |\\n\"\n",
    "        summary_md += f\"| &nbsp; &nbsp; LSTM Layers | {cfg.num_layers} |\\n\"\n",
    "\n",
    "    summary_md += f\"\"\"| | |\n",
    "| **Timings** | |\n",
    "| &nbsp; &nbsp; Total Training Time | {total_training_duration:.2f} s ({total_training_duration/60:.2f} min) |\n",
    "| &nbsp; &nbsp; Average Time per Epoch | {avg_epoch_time:.2f} s |\n",
    "| &nbsp; &nbsp; Average Time per Iteration | {avg_iteration_time:.4f} s |\n",
    "\"\"\"\n",
    "    display(Markdown(summary_md))\n",
    "\n",
    "def show_qualitative_results(run_data, num_examples=3):\n",
    "    if 'generated_captions' not in run_data or 'ground_truths' not in run_data:\n",
    "        print(\"Qualitative results not available.\")\n",
    "        return\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"           QUALITATIVE ANALYSIS: GENERATED CAPTIONS\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    generated_map = {item['image_id']: item['caption'] for item in run_data['generated_captions']}\n",
    "    gt_map = {}\n",
    "    for ann in run_data['ground_truths']['annotations']:\n",
    "        img_id = ann['image_id']\n",
    "        if img_id not in gt_map:\n",
    "            gt_map[img_id] = []\n",
    "        gt_map[img_id].append(ann['caption'])\n",
    "        \n",
    "    image_ids = random.sample(list(generated_map.keys()), min(num_examples, len(generated_map)))\n",
    "    \n",
    "    for image_id in image_ids:\n",
    "        image_path = os.path.join(run_data['config']['image_dir'], image_id)\n",
    "        if not os.path.exists(image_path):\n",
    "            continue\n",
    "            \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        image = Image.open(image_path)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        display(Markdown(f\"**Generated Caption:** `{generated_map[image_id]}`\"))\n",
    "        display(Markdown(\"**Ground Truths:**\"))\n",
    "        for gt_caption in gt_map.get(image_id, []):\n",
    "            display(Markdown(f\"- *{gt_caption}*\"))\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "\n",
    "def generate_final_report(results):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"           FINAL COMPARATIVE ANALYSIS\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    for dataset_name, exps in results.items():\n",
    "        display(Markdown(f'## üìä Results for Dataset: `{dataset_name}`'))\n",
    "        for exp_name, run_data in exps.items():\n",
    "            display(Markdown(f'### üî¨ Experiment: `{exp_name}`'))\n",
    "            \n",
    "            # --- Performance Summary ---\n",
    "            display_performance_summary(run_data)\n",
    "            \n",
    "            # --- Loss Curves ---\n",
    "            plot_loss_curves(run_data.get('history'), exp_name, dataset_name)\n",
    "            \n",
    "            # --- Metrics Table ---\n",
    "            display(Markdown(\"#### Evaluation Metrics\"))\n",
    "            display_metrics_table(run_data.get('metrics'))\n",
    "            \n",
    "            # --- Qualitative Examples ---\n",
    "            if run_data['cfg'].run_evaluation:\n",
    "                show_qualitative_results(run_data)\n",
    "            \n",
    "            sys.stdout.flush() # Ensure outputs are displayed in order\n",
    "            time.sleep(1.0)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if BaseCFG.run_evaluation:\n",
    "        generate_final_report(results_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Final Review Checklist**\n",
    "This checklist confirms that all identified bugs and robustness improvements have been integrated into the script.\n",
    "\n",
    "| ID | Status | Description | Location of Fix |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| 1 | ‚úÖ | Dependencies are explicitly installed (`pycocoevalcap`, `albumentations`, `tabulate`). | Step 1 |\n",
    "| 2 | ‚úÖ | System dependencies (`java`, `zip`) are installed. | Step 1 |\n",
    "| 3 | ‚úÖ | A compatible PyTorch version is forcibly installed to prevent CUDA errors. | Step 1 |\n",
    "| 4 | ‚úÖ | A compatible NumPy version (`<2.0`) is explicitly installed. | Step 1 |\n",
    "| 5 | ‚úÖ | The data processing pipeline correctly parses image filenames from `captions.txt`. | Step 3 |\n",
    "| 6 | ‚úÖ | The `torch.load` call for the vocabulary object uses `weights_only=False`. | Step 8 |\n",
    "| 7 | ‚úÖ | Teacher-forcing slicing (`[:, :-1]`) is handled correctly and consistently in the `Trainer`. | Step 6 |\n",
    "| 8 | ‚úÖ | The GPT-2 `generate` method is correctly passed the `eos_token_id`. | Step 5 |\n",
    "| 9 | ‚úÖ | The ground-truth JSON for evaluation includes the required `\"info\"` key. | Step 7 |\n",
    "| 10 | ‚úÖ | The `base_path` variable is correctly defined in the global scope. | Step 1 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
